{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "mfORIS4dxb0U",
    "outputId": "ed13c4bb-08a5-4866-b22c-6d071484d49a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf  # test if GPU is there. If it prints '' then restart the session and select GPU as runtime\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG5MScDK80zT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "colab_type": "code",
    "id": "OGQ1SCoL1Bh2",
    "outputId": "3a222ba8-7796-4034-e505-14485bcf2a47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-28 08:22:17--  https://github.com/git-lfs/git-lfs/releases/download/v2.4.2/git-lfs-linux-amd64-2.4.2.tar.gz\n",
      "Resolving github.com (github.com)... 140.82.118.3\n",
      "Connecting to github.com (github.com)|140.82.118.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/13021798/3b2fb1fc-62ae-11e8-9816-cb226f383a73?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200628%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200628T082217Z&X-Amz-Expires=300&X-Amz-Signature=69e39231f9756f816089c6ca90521e37660328d6754519d326b2b768a02a4e3b&X-Amz-SignedHeaders=host&actor_id=0&repo_id=13021798&response-content-disposition=attachment%3B%20filename%3Dgit-lfs-linux-amd64-2.4.2.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2020-06-28 08:22:17--  https://github-production-release-asset-2e65be.s3.amazonaws.com/13021798/3b2fb1fc-62ae-11e8-9816-cb226f383a73?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200628%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200628T082217Z&X-Amz-Expires=300&X-Amz-Signature=69e39231f9756f816089c6ca90521e37660328d6754519d326b2b768a02a4e3b&X-Amz-SignedHeaders=host&actor_id=0&repo_id=13021798&response-content-disposition=attachment%3B%20filename%3Dgit-lfs-linux-amd64-2.4.2.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.177.251\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.177.251|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2930483 (2.8M) [application/octet-stream]\n",
      "Saving to: ‘git-lfs.tar.gz’\n",
      "\n",
      "\r",
      "git-lfs.tar.gz        0%[                    ]       0  --.-KB/s               \r",
      "git-lfs.tar.gz        3%[                    ] 109.56K   391KB/s               \r",
      "git-lfs.tar.gz       31%[=====>              ] 891.56K  1.56MB/s               \r",
      "git-lfs.tar.gz      100%[===================>]   2.79M  4.25MB/s    in 0.7s    \n",
      "\n",
      "2020-06-28 08:22:18 (4.25 MB/s) - ‘git-lfs.tar.gz’ saved [2930483/2930483]\n",
      "\n",
      "git-lfs-2.4.2/\n",
      "git-lfs-2.4.2/git-lfs\n",
      "git-lfs-2.4.2/CHANGELOG.md\n",
      "git-lfs-2.4.2/install.sh\n",
      "git-lfs-2.4.2/README.md\n",
      "Error: Failed to call git rev-parse --git-dir --show-toplevel: \"fatal: not a git repository (or any of the parent directories): .git\\n\"\n",
      "Git LFS initialized.\n",
      "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
      "          with new flags from 'git clone'\n",
      "\n",
      "'git clone' has been updated in upstream Git to have comparable\n",
      "speeds to 'git lfs clone'.\n",
      "Cloning into 'deepspeech'...\n",
      "remote: Enumerating objects: 70, done.\u001b[K\n",
      "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
      "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
      "remote: Total 19257 (delta 20), reused 32 (delta 14), pack-reused 19187\u001b[K\n",
      "Receiving objects: 100% (19257/19257), 47.92 MiB | 15.53 MiB/s, done.\n",
      "Resolving deltas: 100% (13134/13134), done.\n",
      "/content/deepspeech\n",
      "Note: checking out 'tags/v0.7.3'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "HEAD is now at 88584941 Merge pull request #3036 from lissyx/doc-fix\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "# clone the repo and checkout to the latest stable branch 0.7.3 to finetune\n",
    "\n",
    "!wget -O git-lfs.tar.gz https://github.com/git-lfs/git-lfs/releases/download/v2.4.2/git-lfs-linux-amd64-2.4.2.tar.gz\n",
    "!tar -xvzf git-lfs.tar.gz\n",
    "!./git-lfs-2.4.2/install.sh\n",
    "!git lfs clone https://github.com/mozilla/deepspeech\n",
    "%cd deepspeech\n",
    "!git checkout -f tags/v0.7.3\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "o1f5x3BuEkB6",
    "outputId": "a2a98708-c89a-4dab-b023-52148d9aa6bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepspeech  deepspeech-colab.ipynb  new_checkpoints  training_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bY5GPoM2Jjgz",
    "outputId": "73e5154d-84c8-48e9-f5db-90b1f065b49d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webrtcvad\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/34/e2de2d97f3288512b9ea56f92e7452f8207eb5a0096500badf9dfd48f5e6/webrtcvad-2.0.10.tar.gz (66kB)\n",
      "\r",
      "\u001b[K     |█████                           | 10kB 26.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 20kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 30kB 5.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 40kB 4.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 51kB 5.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 61kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 71kB 4.4MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: webrtcvad\n",
      "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp36-cp36m-linux_x86_64.whl size=71386 sha256=15522bbca14453379ad6f2589a165d5d5f10731dbb414a96efe7530ade829d0d\n",
      "  Stored in directory: /root/.cache/pip/wheels/44/2a/18/bd1aec41cac7c3051fe95d92a6ed446122ea31dc713c432fa1\n",
      "Successfully built webrtcvad\n",
      "Installing collected packages: webrtcvad\n",
      "Successfully installed webrtcvad-2.0.10\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from -r deepspeech/requirements_tests.txt (line 1)) (0.9.0)\n",
      "Collecting argparse\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
      "Collecting semver\n",
      "  Downloading https://files.pythonhosted.org/packages/94/19/275d10576ad1b19b801efa478182d7352d244a40164162334010e3a948d5/semver-2.10.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->-r deepspeech/requirements_tests.txt (line 1)) (1.12.0)\n",
      "Installing collected packages: argparse, semver\n",
      "Successfully installed argparse-1.4.0 semver-2.10.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "argparse"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r deepspeech/requirements_eval_tflite.txt (line 1)) (0.9.0)\n",
      "Collecting attrdict==2.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/97/28fe7e68bc7adfce67d4339756e85e9fcf3c6fd7f0c0781695352b70472c/attrdict-2.0.1-py2.py3-none-any.whl\n",
      "Collecting deepspeech\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/17/3694ad9f691b7d5f558c48cdc7a1b0c283ecfc22b31d9a8f5ffefb2c307a/deepspeech-0.7.4-cp36-cp36m-manylinux1_x86_64.whl (9.7MB)\n",
      "\u001b[K     |████████████████████████████████| 9.7MB 10.2MB/s \n",
      "\u001b[?25hCollecting numpy==1.16.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/74/54c5f9bb9bd4dae27a61ec1b39076a39d359b3fb7ba15da79ef23858a9d8/numpy-1.16.0-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3MB 203kB/s \n",
      "\u001b[?25hCollecting progressbar2==3.47.0\n",
      "  Downloading https://files.pythonhosted.org/packages/16/68/adc395e0a3c86571081c8a2e2daaa5b58270f6854276a089a0e9b5fa2c33/progressbar2-3.47.0-py2.py3-none-any.whl\n",
      "Collecting python-utils==2.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
      "Collecting six==1.13.0\n",
      "  Downloading https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n",
      "Collecting pandas==0.25.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4MB 55.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3->-r deepspeech/requirements_eval_tflite.txt (line 8)) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3->-r deepspeech/requirements_eval_tflite.txt (line 8)) (2.8.1)\n",
      "\u001b[31mERROR: umap-learn 0.4.4 has requirement numpy>=1.17, but you'll have numpy 1.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.13.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: six, attrdict, numpy, deepspeech, python-utils, progressbar2, pandas\n",
      "  Found existing installation: six 1.12.0\n",
      "    Uninstalling six-1.12.0:\n",
      "      Successfully uninstalled six-1.12.0\n",
      "  Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "  Found existing installation: python-utils 2.4.0\n",
      "    Uninstalling python-utils-2.4.0:\n",
      "      Successfully uninstalled python-utils-2.4.0\n",
      "  Found existing installation: progressbar2 3.38.0\n",
      "    Uninstalling progressbar2-3.38.0:\n",
      "      Successfully uninstalled progressbar2-3.38.0\n",
      "  Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed attrdict-2.0.1 deepspeech-0.7.4 numpy-1.16.0 pandas-0.25.3 progressbar2-3.47.0 python-utils-2.3.0 six-1.13.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy",
         "pandas",
         "six"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepspeech-gpu\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/39/a29a01ebd6700807f654acdc05db844dd0a78f46405acc2ff1295db4f27b/deepspeech_gpu-0.7.4-cp36-cp36m-manylinux1_x86_64.whl (19.2MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2MB 43.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepspeech-gpu) (1.16.0)\n",
      "Installing collected packages: deepspeech-gpu\n",
      "Successfully installed deepspeech-gpu-0.7.4\n"
     ]
    }
   ],
   "source": [
    "#install all the requirements\n",
    "!pip3 install -r deepspeech/requirements_transcribe.txt\n",
    "!pip3 install -r deepspeech/requirements_tests.txt\n",
    "!pip3 install -r deepspeech/requirements_eval_tflite.txt\n",
    "!pip3 install deepspeech-gpu\n",
    "\n",
    "!pip3 uninstall tensorflow -y\n",
    "!pip3 install tensorflow-gpu==1.15.2\n",
    "\n",
    "%cd deepspeech\n",
    "\n",
    "!pip3 install -e .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9mP-5NO0D4xM",
    "outputId": "ffa76d8b-64f0-4e27-92b0-77514e6a9ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-28 08:45:44--  https://www.dropbox.com/sh/y70adhrpgsg5mfz/AABXhHz53V1U6JYy2CDG4eZQa?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.66.1, 2620:100:6022:1::a27d:4201\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.66.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /sh/raw/y70adhrpgsg5mfz/AABXhHz53V1U6JYy2CDG4eZQa [following]\n",
      "--2020-06-28 08:45:45--  https://www.dropbox.com/sh/raw/y70adhrpgsg5mfz/AABXhHz53V1U6JYy2CDG4eZQa\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc88dc799bca3646f92e51ee3fc9.dl.dropboxusercontent.com/zip_download_get/Ad4dVK4GZi-CxVsStd-IGzXWhpSBSEv-gaiLCIT-8UNCptviA87b8Xw_uxLLUJXwHpm6zL2Pr3PMInx_FzPgWbZxzBvsqv7QPSOB4KnlDLxl8g [following]\n",
      "--2020-06-28 08:45:46--  https://uc88dc799bca3646f92e51ee3fc9.dl.dropboxusercontent.com/zip_download_get/Ad4dVK4GZi-CxVsStd-IGzXWhpSBSEv-gaiLCIT-8UNCptviA87b8Xw_uxLLUJXwHpm6zL2Pr3PMInx_FzPgWbZxzBvsqv7QPSOB4KnlDLxl8g\n",
      "Resolving uc88dc799bca3646f92e51ee3fc9.dl.dropboxusercontent.com (uc88dc799bca3646f92e51ee3fc9.dl.dropboxusercontent.com)... 162.125.66.15, 2620:100:6022:15::a27d:420f\n",
      "Connecting to uc88dc799bca3646f92e51ee3fc9.dl.dropboxusercontent.com (uc88dc799bca3646f92e51ee3fc9.dl.dropboxusercontent.com)|162.125.66.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 62729826 (60M) [application/zip]\n",
      "Saving to: ‘AABXhHz53V1U6JYy2CDG4eZQa?dl=0’\n",
      "\n",
      "AABXhHz53V1U6JYy2CD 100%[===================>]  59.82M  2.75MB/s    in 22s     \n",
      "\n",
      "2020-06-28 08:46:08 (2.73 MB/s) - ‘AABXhHz53V1U6JYy2CDG4eZQa?dl=0’ saved [62729826/62729826]\n",
      "\n",
      "Archive:  data.tar\n",
      "warning:  stripped absolute path spec from /\n",
      "mapname:  conversion of  failed\n",
      " extracting: val.csv                 \n",
      " extracting: test.csv                \n",
      " extracting: train.csv               \n",
      "   creating: audio_chunks/\n",
      " extracting: audio_chunks/5.wav      \n",
      " extracting: audio_chunks/3.wav      \n",
      " extracting: audio_chunks/6.wav      \n",
      " extracting: audio_chunks/8.wav      \n",
      " extracting: audio_chunks/0.wav      \n",
      " extracting: audio_chunks/2.wav      \n",
      " extracting: audio_chunks/1.wav      \n",
      " extracting: audio_chunks/9.wav      \n",
      " extracting: audio_chunks/7.wav      \n",
      " extracting: audio_chunks/4.wav      \n",
      " extracting: audio_chunks/47.wav     \n",
      " extracting: audio_chunks/57.wav     \n",
      " extracting: audio_chunks/43.wav     \n",
      " extracting: audio_chunks/12.wav     \n",
      " extracting: audio_chunks/24.wav     \n",
      " extracting: audio_chunks/96.wav     \n",
      " extracting: audio_chunks/54.wav     \n",
      " extracting: audio_chunks/93.wav     \n",
      " extracting: audio_chunks/81.wav     \n",
      " extracting: audio_chunks/37.wav     \n",
      " extracting: audio_chunks/23.wav     \n",
      " extracting: audio_chunks/88.wav     \n",
      " extracting: audio_chunks/72.wav     \n",
      " extracting: audio_chunks/30.wav     \n",
      " extracting: audio_chunks/60.wav     \n",
      " extracting: audio_chunks/68.wav     \n",
      " extracting: audio_chunks/75.wav     \n",
      " extracting: audio_chunks/70.wav     \n",
      " extracting: audio_chunks/35.wav     \n",
      " extracting: audio_chunks/56.wav     \n",
      " extracting: audio_chunks/44.wav     \n",
      " extracting: audio_chunks/59.wav     \n",
      " extracting: audio_chunks/17.wav     \n",
      " extracting: audio_chunks/71.wav     \n",
      " extracting: audio_chunks/48.wav     \n",
      " extracting: audio_chunks/90.wav     \n",
      " extracting: audio_chunks/50.wav     \n",
      " extracting: audio_chunks/36.wav     \n",
      " extracting: audio_chunks/26.wav     \n",
      " extracting: audio_chunks/14.wav     \n",
      " extracting: audio_chunks/51.wav     \n",
      " extracting: audio_chunks/89.wav     \n",
      " extracting: audio_chunks/33.wav     \n",
      " extracting: audio_chunks/85.wav     \n",
      " extracting: audio_chunks/19.wav     \n",
      " extracting: audio_chunks/58.wav     \n",
      " extracting: audio_chunks/27.wav     \n",
      " extracting: audio_chunks/20.wav     \n",
      " extracting: audio_chunks/79.wav     \n",
      " extracting: audio_chunks/61.wav     \n",
      " extracting: audio_chunks/11.wav     \n",
      " extracting: audio_chunks/99.wav     \n",
      " extracting: audio_chunks/87.wav     \n",
      " extracting: audio_chunks/53.wav     \n",
      " extracting: audio_chunks/21.wav     \n",
      " extracting: audio_chunks/25.wav     \n",
      " extracting: audio_chunks/83.wav     \n",
      " extracting: audio_chunks/62.wav     \n",
      " extracting: audio_chunks/55.wav     \n",
      " extracting: audio_chunks/66.wav     \n",
      " extracting: audio_chunks/67.wav     \n",
      " extracting: audio_chunks/34.wav     \n",
      " extracting: audio_chunks/39.wav     \n",
      " extracting: audio_chunks/69.wav     \n",
      " extracting: audio_chunks/74.wav     \n",
      " extracting: audio_chunks/16.wav     \n",
      " extracting: audio_chunks/42.wav     \n",
      " extracting: audio_chunks/28.wav     \n",
      " extracting: audio_chunks/92.wav     \n",
      " extracting: audio_chunks/13.wav     \n",
      " extracting: audio_chunks/31.wav     \n",
      " extracting: audio_chunks/73.wav     \n",
      " extracting: audio_chunks/45.wav     \n",
      " extracting: audio_chunks/80.wav     \n",
      " extracting: audio_chunks/22.wav     \n",
      " extracting: audio_chunks/63.wav     \n",
      " extracting: audio_chunks/98.wav     \n",
      " extracting: audio_chunks/64.wav     \n",
      " extracting: audio_chunks/78.wav     \n",
      " extracting: audio_chunks/38.wav     \n",
      " extracting: audio_chunks/29.wav     \n",
      " extracting: audio_chunks/10.wav     \n",
      " extracting: audio_chunks/77.wav     \n",
      " extracting: audio_chunks/15.wav     \n",
      " extracting: audio_chunks/18.wav     \n",
      " extracting: audio_chunks/49.wav     \n",
      " extracting: audio_chunks/95.wav     \n",
      " extracting: audio_chunks/91.wav     \n",
      " extracting: audio_chunks/65.wav     \n",
      " extracting: audio_chunks/84.wav     \n",
      " extracting: audio_chunks/40.wav     \n",
      " extracting: audio_chunks/32.wav     \n",
      " extracting: audio_chunks/97.wav     \n",
      " extracting: audio_chunks/76.wav     \n",
      " extracting: audio_chunks/86.wav     \n",
      " extracting: audio_chunks/82.wav     \n",
      " extracting: audio_chunks/41.wav     \n",
      " extracting: audio_chunks/94.wav     \n",
      " extracting: audio_chunks/46.wav     \n",
      " extracting: audio_chunks/52.wav     \n",
      " extracting: audio_chunks/185.wav    \n",
      " extracting: audio_chunks/104.wav    \n",
      " extracting: audio_chunks/258.wav    \n",
      " extracting: audio_chunks/227.wav    \n",
      " extracting: audio_chunks/245.wav    \n",
      " extracting: audio_chunks/167.wav    \n",
      " extracting: audio_chunks/230.wav    \n",
      " extracting: audio_chunks/143.wav    \n",
      " extracting: audio_chunks/109.wav    \n",
      " extracting: audio_chunks/148.wav    \n",
      " extracting: audio_chunks/360.wav    \n",
      " extracting: audio_chunks/309.wav    \n",
      " extracting: audio_chunks/263.wav    \n",
      " extracting: audio_chunks/255.wav    \n",
      " extracting: audio_chunks/205.wav    \n",
      " extracting: audio_chunks/119.wav    \n",
      " extracting: audio_chunks/136.wav    \n",
      " extracting: audio_chunks/266.wav    \n",
      " extracting: audio_chunks/160.wav    \n",
      " extracting: audio_chunks/187.wav    \n",
      " extracting: audio_chunks/141.wav    \n",
      " extracting: audio_chunks/284.wav    \n",
      " extracting: audio_chunks/145.wav    \n",
      " extracting: audio_chunks/123.wav    \n",
      " extracting: audio_chunks/240.wav    \n",
      " extracting: audio_chunks/351.wav    \n",
      " extracting: audio_chunks/218.wav    \n",
      " extracting: audio_chunks/271.wav    \n",
      " extracting: audio_chunks/135.wav    \n",
      " extracting: audio_chunks/208.wav    \n",
      " extracting: audio_chunks/117.wav    \n",
      " extracting: audio_chunks/207.wav    \n",
      " extracting: audio_chunks/252.wav    \n",
      " extracting: audio_chunks/178.wav    \n",
      " extracting: audio_chunks/297.wav    \n",
      " extracting: audio_chunks/107.wav    \n",
      " extracting: audio_chunks/293.wav    \n",
      " extracting: audio_chunks/129.wav    \n",
      " extracting: audio_chunks/181.wav    \n",
      " extracting: audio_chunks/257.wav    \n",
      " extracting: audio_chunks/246.wav    \n",
      " extracting: audio_chunks/213.wav    \n",
      " extracting: audio_chunks/111.wav    \n",
      " extracting: audio_chunks/329.wav    \n",
      " extracting: audio_chunks/234.wav    \n",
      " extracting: audio_chunks/108.wav    \n",
      " extracting: audio_chunks/177.wav    \n",
      " extracting: audio_chunks/137.wav    \n",
      " extracting: audio_chunks/101.wav    \n",
      " extracting: audio_chunks/289.wav    \n",
      " extracting: audio_chunks/310.wav    \n",
      " extracting: audio_chunks/249.wav    \n",
      " extracting: audio_chunks/184.wav    \n",
      " extracting: audio_chunks/302.wav    \n",
      " extracting: audio_chunks/132.wav    \n",
      " extracting: audio_chunks/322.wav    \n",
      " extracting: audio_chunks/146.wav    \n",
      " extracting: audio_chunks/158.wav    \n",
      " extracting: audio_chunks/365.wav    \n",
      " extracting: audio_chunks/114.wav    \n",
      " extracting: audio_chunks/280.wav    \n",
      " extracting: audio_chunks/163.wav    \n",
      " extracting: audio_chunks/254.wav    \n",
      " extracting: audio_chunks/196.wav    \n",
      " extracting: audio_chunks/179.wav    \n",
      " extracting: audio_chunks/296.wav    \n",
      " extracting: audio_chunks/244.wav    \n",
      " extracting: audio_chunks/106.wav    \n",
      " extracting: audio_chunks/124.wav    \n",
      " extracting: audio_chunks/268.wav    \n",
      " extracting: audio_chunks/332.wav    \n",
      " extracting: audio_chunks/336.wav    \n",
      " extracting: audio_chunks/358.wav    \n",
      " extracting: audio_chunks/320.wav    \n",
      " extracting: audio_chunks/173.wav    \n",
      " extracting: audio_chunks/103.wav    \n",
      " extracting: audio_chunks/342.wav    \n",
      " extracting: audio_chunks/112.wav    \n",
      " extracting: audio_chunks/128.wav    \n",
      " extracting: audio_chunks/283.wav    \n",
      " extracting: audio_chunks/191.wav    \n",
      " extracting: audio_chunks/265.wav    \n",
      " extracting: audio_chunks/306.wav    \n",
      " extracting: audio_chunks/155.wav    \n",
      " extracting: audio_chunks/224.wav    \n",
      " extracting: audio_chunks/330.wav    \n",
      " extracting: audio_chunks/324.wav    \n",
      " extracting: audio_chunks/217.wav    \n",
      " extracting: audio_chunks/295.wav    \n",
      " extracting: audio_chunks/275.wav    \n",
      " extracting: audio_chunks/237.wav    \n",
      " extracting: audio_chunks/100.wav    \n",
      " extracting: audio_chunks/169.wav    \n",
      " extracting: audio_chunks/333.wav    \n",
      " extracting: audio_chunks/110.wav    \n",
      " extracting: audio_chunks/200.wav    \n",
      " extracting: audio_chunks/362.wav    \n",
      " extracting: audio_chunks/341.wav    \n",
      " extracting: audio_chunks/311.wav    \n",
      " extracting: audio_chunks/314.wav    \n",
      " extracting: audio_chunks/339.wav    \n",
      " extracting: audio_chunks/203.wav    \n",
      " extracting: audio_chunks/292.wav    \n",
      " extracting: audio_chunks/277.wav    \n",
      " extracting: audio_chunks/273.wav    \n",
      " extracting: audio_chunks/251.wav    \n",
      " extracting: audio_chunks/215.wav    \n",
      " extracting: audio_chunks/131.wav    \n",
      " extracting: audio_chunks/225.wav    \n",
      " extracting: audio_chunks/331.wav    \n",
      " extracting: audio_chunks/347.wav    \n",
      " extracting: audio_chunks/353.wav    \n",
      " extracting: audio_chunks/239.wav    \n",
      " extracting: audio_chunks/269.wav    \n",
      " extracting: audio_chunks/334.wav    \n",
      " extracting: audio_chunks/308.wav    \n",
      " extracting: audio_chunks/232.wav    \n",
      " extracting: audio_chunks/223.wav    \n",
      " extracting: audio_chunks/149.wav    \n",
      " extracting: audio_chunks/216.wav    \n",
      " extracting: audio_chunks/312.wav    \n",
      " extracting: audio_chunks/327.wav    \n",
      " extracting: audio_chunks/236.wav    \n",
      " extracting: audio_chunks/281.wav    \n",
      " extracting: audio_chunks/340.wav    \n",
      " extracting: audio_chunks/170.wav    \n",
      " extracting: audio_chunks/305.wav    \n",
      " extracting: audio_chunks/162.wav    \n",
      " extracting: audio_chunks/298.wav    \n",
      " extracting: audio_chunks/301.wav    \n",
      " extracting: audio_chunks/345.wav    \n",
      " extracting: audio_chunks/142.wav    \n",
      " extracting: audio_chunks/270.wav    \n",
      " extracting: audio_chunks/105.wav    \n",
      " extracting: audio_chunks/359.wav    \n",
      " extracting: audio_chunks/259.wav    \n",
      " extracting: audio_chunks/226.wav    \n",
      " extracting: audio_chunks/291.wav    \n",
      " extracting: audio_chunks/194.wav    \n",
      " extracting: audio_chunks/238.wav    \n",
      " extracting: audio_chunks/256.wav    \n",
      " extracting: audio_chunks/152.wav    \n",
      " extracting: audio_chunks/156.wav    \n",
      " extracting: audio_chunks/228.wav    \n",
      " extracting: audio_chunks/279.wav    \n",
      " extracting: audio_chunks/138.wav    \n",
      " extracting: audio_chunks/113.wav    \n",
      " extracting: audio_chunks/140.wav    \n",
      " extracting: audio_chunks/285.wav    \n",
      " extracting: audio_chunks/122.wav    \n",
      " extracting: audio_chunks/220.wav    \n",
      " extracting: audio_chunks/241.wav    \n",
      " extracting: audio_chunks/198.wav    \n",
      " extracting: audio_chunks/212.wav    \n",
      " extracting: audio_chunks/199.wav    \n",
      " extracting: audio_chunks/195.wav    \n",
      " extracting: audio_chunks/316.wav    \n",
      " extracting: audio_chunks/166.wav    \n",
      " extracting: audio_chunks/346.wav    \n",
      " extracting: audio_chunks/304.wav    \n",
      " extracting: audio_chunks/192.wav    \n",
      " extracting: audio_chunks/118.wav    \n",
      " extracting: audio_chunks/267.wav    \n",
      " extracting: audio_chunks/150.wav    \n",
      " extracting: audio_chunks/186.wav    \n",
      " extracting: audio_chunks/144.wav    \n",
      " extracting: audio_chunks/247.wav    \n",
      " extracting: audio_chunks/288.wav    \n",
      " extracting: audio_chunks/193.wav    \n",
      " extracting: audio_chunks/183.wav    \n",
      " extracting: audio_chunks/197.wav    \n",
      " extracting: audio_chunks/315.wav    \n",
      " extracting: audio_chunks/337.wav    \n",
      " extracting: audio_chunks/153.wav    \n",
      " extracting: audio_chunks/222.wav    \n",
      " extracting: audio_chunks/180.wav    \n",
      " extracting: audio_chunks/102.wav    \n",
      " extracting: audio_chunks/354.wav    \n",
      " extracting: audio_chunks/350.wav    \n",
      " extracting: audio_chunks/121.wav    \n",
      " extracting: audio_chunks/171.wav    \n",
      " extracting: audio_chunks/174.wav    \n",
      " extracting: audio_chunks/159.wav    \n",
      " extracting: audio_chunks/364.wav    \n",
      " extracting: audio_chunks/235.wav    \n",
      " extracting: audio_chunks/115.wav    \n",
      " extracting: audio_chunks/307.wav    \n",
      " extracting: audio_chunks/189.wav    \n",
      " extracting: audio_chunks/323.wav    \n",
      " extracting: audio_chunks/328.wav    \n",
      " extracting: audio_chunks/151.wav    \n",
      " extracting: audio_chunks/127.wav    \n",
      " extracting: audio_chunks/133.wav    \n",
      " extracting: audio_chunks/356.wav    \n",
      " extracting: audio_chunks/154.wav    \n",
      " extracting: audio_chunks/300.wav    \n",
      " extracting: audio_chunks/168.wav    \n",
      " extracting: audio_chunks/321.wav    \n",
      " extracting: audio_chunks/172.wav    \n",
      " extracting: audio_chunks/325.wav    \n",
      " extracting: audio_chunks/343.wav    \n",
      " extracting: audio_chunks/211.wav    \n",
      " extracting: audio_chunks/204.wav    \n",
      " extracting: audio_chunks/147.wav    \n",
      " extracting: audio_chunks/355.wav    \n",
      " extracting: audio_chunks/120.wav    \n",
      " extracting: audio_chunks/286.wav    \n",
      " extracting: audio_chunks/201.wav    \n",
      " extracting: audio_chunks/318.wav    \n",
      " extracting: audio_chunks/165.wav    \n",
      " extracting: audio_chunks/290.wav    \n",
      " extracting: audio_chunks/182.wav    \n",
      " extracting: audio_chunks/161.wav    \n",
      " extracting: audio_chunks/349.wav    \n",
      " extracting: audio_chunks/125.wav    \n",
      " extracting: audio_chunks/278.wav    \n",
      " extracting: audio_chunks/274.wav    \n",
      " extracting: audio_chunks/139.wav    \n",
      " extracting: audio_chunks/317.wav    \n",
      " extracting: audio_chunks/357.wav    \n",
      " extracting: audio_chunks/361.wav    \n",
      " extracting: audio_chunks/190.wav    \n",
      " extracting: audio_chunks/260.wav    \n",
      " extracting: audio_chunks/264.wav    \n",
      " extracting: audio_chunks/242.wav    \n",
      " extracting: audio_chunks/231.wav    \n",
      " extracting: audio_chunks/221.wav    \n",
      " extracting: audio_chunks/352.wav    \n",
      " extracting: audio_chunks/276.wav    \n",
      " extracting: audio_chunks/250.wav    \n",
      " extracting: audio_chunks/326.wav    \n",
      " extracting: audio_chunks/214.wav    \n",
      " extracting: audio_chunks/209.wav    \n",
      " extracting: audio_chunks/338.wav    \n",
      " extracting: audio_chunks/176.wav    \n",
      " extracting: audio_chunks/294.wav    \n",
      " extracting: audio_chunks/248.wav    \n",
      " extracting: audio_chunks/303.wav    \n",
      " extracting: audio_chunks/126.wav    \n",
      " extracting: audio_chunks/319.wav    \n",
      " extracting: audio_chunks/164.wav    \n",
      " extracting: audio_chunks/210.wav    \n",
      " extracting: audio_chunks/219.wav    \n",
      " extracting: audio_chunks/344.wav    \n",
      " extracting: audio_chunks/262.wav    \n",
      " extracting: audio_chunks/272.wav    \n",
      " extracting: audio_chunks/313.wav    \n",
      " extracting: audio_chunks/229.wav    \n",
      " extracting: audio_chunks/363.wav    \n",
      " extracting: audio_chunks/348.wav    \n",
      " extracting: audio_chunks/299.wav    \n",
      " extracting: audio_chunks/175.wav    \n",
      " extracting: audio_chunks/233.wav    \n",
      " extracting: audio_chunks/157.wav    \n",
      " extracting: audio_chunks/261.wav    \n",
      " extracting: audio_chunks/243.wav    \n",
      " extracting: audio_chunks/188.wav    \n",
      " extracting: audio_chunks/335.wav    \n",
      " extracting: audio_chunks/134.wav    \n",
      " extracting: audio_chunks/130.wav    \n",
      " extracting: audio_chunks/116.wav    \n",
      " extracting: audio_chunks/206.wav    \n",
      " extracting: audio_chunks/253.wav    \n",
      " extracting: audio_chunks/202.wav    \n",
      " extracting: audio_chunks/287.wav    \n",
      " extracting: audio_chunks/282.wav    \n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n# download and extract checkpoints\\n!rm -r deepspeech/new_checkpoints\\n!wget https://www.dropbox.com/sh/i76w40hmb1m5jpi/AAC_eUJXRutNnrNaGHWUINnYa?dl=0\\n!mv 'AAC_eUJXRutNnrNaGHWUINnYa?dl=0' checkpoints.tar\\n!mkdir new_checkpoints/\\n!unzip checkpoints.tar\\n!cp {best_dev-732522.data-00000-of-00001,best_dev-732522.index,best_dev-732522.meta,best_dev_checkpoint} new_checkpoints\\n\\n!cp -r new_checkpoints deepspeech\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# download and extract training data\n",
    "!wget https://www.dropbox.com/sh/y70adhrpgsg5mfz/AABXhHz53V1U6JYy2CDG4eZQa?dl=0\n",
    "!mv 'AABXhHz53V1U6JYy2CDG4eZQa?dl=0' data.tar\n",
    "!unzip data.tar\n",
    "!mkdir training_data\n",
    "!cp -r {audio_chunks,train.csv,test.csv,val.csv} training_data  # copy files at relevant path\n",
    "\n",
    "!cp -r training_data deepspeech\n",
    "\n",
    "\n",
    "\n",
    "# download and extract latest stable checkpoints for the exact branch i.e. 0.7.3\n",
    "!rm -r deepspeech/new_checkpoints\n",
    "!wget https://www.dropbox.com/sh/i76w40hmb1m5jpi/AAC_eUJXRutNnrNaGHWUINnYa?dl=0\n",
    "!mv 'AAC_eUJXRutNnrNaGHWUINnYa?dl=0' checkpoints.tar\n",
    "!mkdir new_checkpoints/\n",
    "!unzip checkpoints.tar\n",
    "!cp {best_dev-732522.data-00000-of-00001,best_dev-732522.index,best_dev-732522.meta,best_dev_checkpoint} new_checkpoints\n",
    "\n",
    "!cp -r new_checkpoints deepspeech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "79uMqWtS8aMj",
    "outputId": "27a394ec-c74e-4021-c4bd-45f1ea735a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/deepspeech\n"
     ]
    }
   ],
   "source": [
    "# to store the fine tuned model\n",
    "%cd deepspeech\n",
    "!mkdir fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Vh6DaxUb4CM-",
    "outputId": "1cf2ce19-dbd4-4d76-c9b1-de3b982a69a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/deepspeech\n"
     ]
    }
   ],
   "source": [
    "%cd deepspeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zBWCgLfAqGm2",
    "outputId": "0042607f-5674-4c7e-ebc5-495774a27859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swig/python detected a memory leak of type 'Alphabet *', no destructor found.\n",
      "I Loading best validating checkpoint from new_checkpoints/best_dev-732522\n",
      "I Loading variable from checkpoint: beta1_power\n",
      "I Loading variable from checkpoint: beta2_power\n",
      "I Loading variable from checkpoint: cudnn_lstm/opaque_kernel\n",
      "I Loading variable from checkpoint: cudnn_lstm/opaque_kernel/Adam\n",
      "I Loading variable from checkpoint: cudnn_lstm/opaque_kernel/Adam_1\n",
      "I Loading variable from checkpoint: global_step\n",
      "I Loading variable from checkpoint: layer_1/bias\n",
      "I Loading variable from checkpoint: layer_1/bias/Adam\n",
      "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
      "I Loading variable from checkpoint: layer_1/weights\n",
      "I Loading variable from checkpoint: layer_1/weights/Adam\n",
      "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
      "I Loading variable from checkpoint: layer_2/bias\n",
      "I Loading variable from checkpoint: layer_2/bias/Adam\n",
      "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
      "I Loading variable from checkpoint: layer_2/weights\n",
      "I Loading variable from checkpoint: layer_2/weights/Adam\n",
      "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
      "I Loading variable from checkpoint: layer_3/bias\n",
      "I Loading variable from checkpoint: layer_3/bias/Adam\n",
      "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
      "I Loading variable from checkpoint: layer_3/weights\n",
      "I Loading variable from checkpoint: layer_3/weights/Adam\n",
      "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
      "I Loading variable from checkpoint: layer_5/bias\n",
      "I Loading variable from checkpoint: layer_5/bias/Adam\n",
      "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
      "I Loading variable from checkpoint: layer_5/weights\n",
      "I Loading variable from checkpoint: layer_5/weights/Adam\n",
      "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
      "I Loading variable from checkpoint: layer_6/bias\n",
      "I Loading variable from checkpoint: layer_6/bias/Adam\n",
      "I Loading variable from checkpoint: layer_6/bias/Adam_1\n",
      "I Loading variable from checkpoint: layer_6/weights\n",
      "I Loading variable from checkpoint: layer_6/weights/Adam\n",
      "I Loading variable from checkpoint: layer_6/weights/Adam_1\n",
      "I Initializing variable: learning_rate\n",
      "I STARTING Optimization\n",
      "Epoch 0 |   Training | Elapsed Time: 0:00:23 | Steps: 43 | Loss: 236.905569     \n",
      "Epoch 0 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 277.193668 | Dataset: training_data/val.csv\n",
      "I Saved new best validating model with loss 277.193668 to: new_checkpoints/best_dev-732565\n",
      "Epoch 1 |   Training | Elapsed Time: 0:00:21 | Steps: 43 | Loss: 221.531122     \n",
      "Epoch 1 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 276.300247 | Dataset: training_data/val.csv\n",
      "I Saved new best validating model with loss 276.300247 to: new_checkpoints/best_dev-732608\n",
      "Epoch 2 |   Training | Elapsed Time: 0:00:20 | Steps: 43 | Loss: 212.080758     \n",
      "Epoch 2 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 272.107621 | Dataset: training_data/val.csv\n",
      "I Saved new best validating model with loss 272.107621 to: new_checkpoints/best_dev-732651\n",
      "Epoch 3 |   Training | Elapsed Time: 0:00:20 | Steps: 43 | Loss: 203.354731     \n",
      "Epoch 3 | Validation | Elapsed Time: 0:00:01 | Steps: 10 | Loss: 263.682386 | Dataset: training_data/val.csv\n",
      "I Saved new best validating model with loss 263.682386 to: new_checkpoints/best_dev-732694\n",
      "Epoch 4 |   Training | Elapsed Time: 0:00:02 | Steps: 7 | Loss: 169.367691      "
     ]
    }
   ],
   "source": [
    "# execute finetuning on the GPU\n",
    "!python3 DeepSpeech.py --train_cudnn true --n_hidden 2048 --dropout_rate 0.30 --epochs 50 --checkpoint_dir new_checkpoints --train_batch_size 8 --learning_rate 0.0001 --train_files training_data/train.csv --export_dir fine_tuned_model --dev_files training_data/val.csv --test_files training_data/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "deepspeech-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
